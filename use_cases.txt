U0. List of facilities that failed their most recent inspections (date and DBA Name are both complete).

U0. Trend of pass/fail rate over time. Date is complete, but the existence of "No Entry"
values in the Results column represents unrecoverable data; gap is as high as 5% for 2017.
However, there's no reason to believe the missing data would bias the analysis (for example,
because it's geographically linked to high-crime areas where inspectors are wary to go).
A simple statistical analysis of both the good and missing datasets could rule out bias.

U1. Pass/Fail + Risk Level heat map of the city. Missing location data prevents that.
Cleaning would involve normalizing the name of the city, then leveraging Google Maps
API to recover the Zip and Lat / Long. It can also be used as a foreign-key constraint
for the data that does exist, in that the existing Lat / Long and Zip must roughly match.

U1. Most common violations now or over time. Format of the record prevents that,
in that it's not normalized. The resulting structure would require splitting the single
Violations column into N columns (with many possibly null for most records), and then
pivoting to M rows for any non-null columns.

U1. Multi-class classifier (Violations) based on location and facility type would involve
elements of each, above. Even something very simplistic (kNN for "should I eat in here?")
would be interesting to show, graphically, on a map. The pop-up dialog would be the
probability of any particular violation occuring, culminating in the probility of a Fail.
The use case would be for a restaurant for which there *is* no data.

U2. Attempting to categorize the most common remediation steps. Concretely, while the violations
are normalized and categorical (making them amenable to encoding and summarization), the
"comments" section that provides notes on necessary remediation is completely free-form.
For example: "NO DISH WASHING FACILITIES ON SITE, (NO THREE COMPARTMENT SINK, WITH GREASE TRAP, OR DISHMACHINE), INSTRUCTED TO PROVIDE,"
It's very unlikely that anything short of a large language model (NLP) could provide any
meaningful insights, and, even then, there's a limit to what it could do. In the course of
learning an embedding for comments for the sake of building a classifier, or for clustering,
it may learn a latent representation for "NO DISH WASHING FACILITIES ON SITE", but extracting
that for human consumption would be exceptionally challenging, and, more importantly wouldn't
be helped by any data cleaning.

U2. Similarly for any analysis involving "Inspection Type". While mostly categorical, there are
edge cases like values of "finish complaint inspection from 5-18-10" that simply can't be clustered.
